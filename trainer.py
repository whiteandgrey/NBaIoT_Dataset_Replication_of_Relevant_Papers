"""
è®­ç»ƒæ¨¡å— - å«è¶…å‚æ•°ä¼˜åŒ–å’Œæ—©åœ
"""
import os
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard
import json


class AutoencoderTrainer:
    """è‡ªç¼–ç å™¨è®­ç»ƒå™¨"""

    def __init__(self, config, device_name):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: é…ç½®å¯¹è±¡
            device_name: è®¾å¤‡åç§°
        """
        self.config = config
        self.device_name = device_name
        self.model = None
        self.history = {}

        # è®­ç»ƒå†å²è®°å½•
        self.training_history = {
            'initial_train': None,
            'hyperparameter_tuning': [],
            'final_train': None,
            'best_params': None,
            'best_val_loss': float('inf')
        }

        # åˆ›å»ºè®¾å¤‡ç‰¹å®šçš„è¾“å‡ºç›®å½•
        self.device_output_dir = os.path.join(config.OUTPUT_DIR, device_name)
        os.makedirs(self.device_output_dir, exist_ok=True)

        print(f"âœ… Trainer initialized for device: {device_name}")
        print(f"   Output directory: {self.device_output_dir}")

    def create_callbacks(self, monitor='val_loss', mode='min',
                         patience=None, save_best_only=True):
        """
        åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°

        Args:
            monitor: ç›‘æ§æŒ‡æ ‡
            mode: ç›‘æ§æ¨¡å¼ï¼ˆ'min'æˆ–'max'ï¼‰
            patience: æ—©åœè€å¿ƒå€¼
            save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹

        Returns:
            å›è°ƒå‡½æ•°åˆ—è¡¨
        """
        if patience is None:
            patience = self.config.EARLY_STOPPING_PATIENCE

        callbacks = []

        # æ—©åœå›è°ƒ
        early_stopping = EarlyStopping(
            monitor=monitor,
            patience=patience,
            mode=mode,
            min_delta=self.config.MIN_DELTA,
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)

        # å­¦ä¹ ç‡è°ƒæ•´å›è°ƒ
        reduce_lr = ReduceLROnPlateau(
            monitor=monitor,
            factor=self.config.REDUCE_LR_FACTOR,
            patience=self.config.REDUCE_LR_PATIENCE,
            min_lr=1e-6,
            mode=mode,
            verbose=1
        )
        callbacks.append(reduce_lr)

        # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ï¼‰
        if self.config.SAVE_MODEL:
            model_checkpoint = ModelCheckpoint(
                filepath=os.path.join(self.device_output_dir, 'best_model.h5'),
                monitor=monitor,
                save_best_only=save_best_only,
                mode=mode,
                verbose=1
            )
            callbacks.append(model_checkpoint)

        # TensorBoardå›è°ƒï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ï¼‰
        if self.config.SAVE_TENSORBOARD_LOGS:
            try:
                tensorboard_dir = os.path.join(self.device_output_dir, 'tensorboard_logs')
                os.makedirs(tensorboard_dir, exist_ok=True)

                tensorboard = TensorBoard(
                    log_dir=tensorboard_dir,
                    histogram_freq=1,
                    write_graph=True,
                    write_images=False,
                    update_freq='epoch'
                )
                callbacks.append(tensorboard)
            except Exception as e:
                print(f"âš ï¸ TensorBoard callback error: {e}")

        return callbacks

    def train(self, train_data, val_data, model=None,
              learning_rate=None, epochs=None, batch_size=None,
              phase_name="Training", verbose=1):
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            train_data: è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ˜¯(X_train, y_train)æˆ–tf.data.Dataset
            val_data: éªŒè¯æ•°æ®
            model: è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.model
            learning_rate: å­¦ä¹ ç‡
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°ï¼ˆä»…ç”¨äºNumPyæ•°æ®ï¼‰
            phase_name: è®­ç»ƒé˜¶æ®µåç§°
            verbose: è¯¦ç»†ç¨‹åº¦

        Returns:
            è®­ç»ƒå†å²å’Œæœ€ä½³éªŒè¯æŸå¤±
        """
        print(f"\n{'=' * 60}")
        print(f"PHASE: {phase_name}")
        print(f"Device: {self.device_name}")
        print(f"{'=' * 60}")

        start_time = time.time()

        # è®¾ç½®å‚æ•°
        if model is None:
            model = self.model
        if learning_rate is None:
            learning_rate = self.config.DEFAULT_LEARNING_RATE
        if epochs is None:
            epochs = self.config.DEFAULT_EPOCHS

        # ç¼–è¯‘æ¨¡å‹
        if hasattr(model, 'compile'):
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                loss='mse',
                metrics=['mae']
            )

        # åˆ›å»ºå›è°ƒå‡½æ•°
        callbacks = self.create_callbacks(monitor='val_loss', mode='min')

        # è®­ç»ƒæ¨¡å‹
        print(f"ğŸ”§ Training parameters:")
        print(f"   Learning rate: {learning_rate:.6f}")
        print(f"   Epochs: {epochs}")

        # æ£€æŸ¥æ•°æ®ç±»å‹
        if isinstance(train_data, tuple) and isinstance(val_data, tuple):
            # NumPyæ•°æ®
            X_train, y_train = train_data
            X_val, y_val = val_data

            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size or self.config.DEFAULT_BATCH_SIZE,
                callbacks=callbacks,
                verbose=verbose
            )
        else:
            # TensorFlowæ•°æ®é›†
            history = model.fit(
                train_data,
                validation_data=val_data,
                epochs=epochs,
                callbacks=callbacks,
                verbose=verbose
            )

        training_time = time.time() - start_time

        # è·å–å†å²è®°å½•å­—å…¸
        history_dict = history.history if hasattr(history, 'history') else history

        # è·å–æœ€ä½³éªŒè¯æŸå¤±
        if 'val_loss' in history_dict:
            best_val_loss = min(history_dict['val_loss'])
        else:
            best_val_loss = history_dict['loss'][-1] if history_dict['loss'] else float('inf')

        print(f"\nğŸ“Š {phase_name} completed:")
        print(f"   Best val loss: {best_val_loss:.6f}")
        print(f"   Training time: {training_time:.2f} seconds")
        print(f"   Total epochs trained: {len(history_dict['loss'])}")

        return history_dict, best_val_loss, training_time

    def initial_training(self, train_data, val_data, learning_rate=0.001, epochs=100):
        """
        åˆå§‹è®­ç»ƒé˜¶æ®µ

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            learning_rate: å­¦ä¹ ç‡
            epochs: epochæ•°

        Returns:
            æœ€ä½³éªŒè¯æŸå¤±
        """
        print(f"\n{'=' * 60}")
        print(f"INITIAL TRAINING PHASE")
        print(f"Device: {self.device_name}")
        print(f"{'=' * 60}")

        # åˆ›å»ºæ¨¡å‹
        from model import Autoencoder
        autoencoder = Autoencoder(self.config)
        model = autoencoder.build()
        self.model = model

        history_dict, best_val_loss, training_time = self.train(
            train_data=train_data,
            val_data=val_data,
            model=model,
            learning_rate=learning_rate,
            epochs=epochs,
            phase_name="Initial Training",
            verbose=self.config.VERBOSE
        )

        # è®°å½•è®­ç»ƒå†å²
        self.training_history['initial_train'] = {
            'history': history_dict,
            'training_time': training_time,
            'best_val_loss': best_val_loss
        }

        return best_val_loss

    def hyperparameter_tuning(self, train_data, val_data):
        """
        è¶…å‚æ•°è°ƒä¼˜é˜¶æ®µ

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®

        Returns:
            æœ€ä½³è¶…å‚æ•°
        """
        print(f"\n{'=' * 60}")
        print(f"HYPERPARAMETER TUNING PHASE")
        print(f"Device: {self.device_name}")
        print(f"{'=' * 60}")

        results = []
        best_val_loss = float('inf')
        best_params = None

        # éå†è¶…å‚æ•°ç»„åˆ
        for lr in self.config.LEARNING_RATES:
            for epochs in self.config.EPOCHS_OPTIONS:
                print(f"\nğŸ§ª Testing: LR={lr:.6f}, Epochs={epochs}")

                # é‡æ–°åˆ›å»ºæ¨¡å‹
                from model import Autoencoder
                autoencoder = Autoencoder(self.config)
                model = autoencoder.build()

                # è®­ç»ƒå¹¶è¯„ä¼°
                history_dict, val_loss, tuning_time = self.train(
                    train_data=train_data,
                    val_data=val_data,
                    model=model,
                    learning_rate=lr,
                    epochs=epochs,
                    phase_name=f"Tuning LR={lr:.4f}, Epochs={epochs}",
                    verbose=0  # é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º
                )

                # è®°å½•ç»“æœ
                result = {
                    'lr': lr,
                    'epochs': epochs,
                    'val_loss': val_loss,
                    'training_time': tuning_time
                }
                results.append(result)

                print(f"   Result: Val Loss={val_loss:.6f}, Time={tuning_time:.2f}s")

                # æ›´æ–°æœ€ä½³å‚æ•°
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_params = {'lr': lr, 'epochs': epochs}

        # ä¿å­˜åˆ°è®­ç»ƒå†å²
        self.training_history['hyperparameter_tuning'] = results
        self.training_history['best_params'] = best_params
        self.training_history['best_val_loss'] = best_val_loss

        # ä¿å­˜è°ƒä¼˜ç»“æœï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ï¼‰
        if self.config.SAVE_HYPERPARAMETER_TUNING_RESULTS:
            tuning_results_path = os.path.join(self.device_output_dir, 'hyperparameter_tuning.json')
            with open(tuning_results_path, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"âœ… Hyperparameter tuning results saved to: {tuning_results_path}")

        print(f"\nâœ… Hyperparameter tuning completed!")
        print(f"   Best parameters: LR={best_params['lr']:.6f}, Epochs={best_params['epochs']}")
        print(f"   Best validation loss: {best_val_loss:.6f}")

        return best_params

    def final_training(self, train_data, val_data=None):
        """
        æœ€ç»ˆè®­ç»ƒé˜¶æ®µï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            æœ€ç»ˆè®­ç»ƒæŸå¤±
        """
        print(f"\n{'=' * 60}")
        print(f"FINAL TRAINING PHASE")
        print(f"Device: {self.device_name}")
        print(f"{'=' * 60}")

        # è·å–æœ€ä½³å‚æ•°
        if self.training_history['best_params'] is None:
            print("âš ï¸ No best parameters found, using defaults")
            best_params = {
                'lr': self.config.DEFAULT_LEARNING_RATE,
                'epochs': self.config.DEFAULT_EPOCHS
            }
        else:
            best_params = self.training_history['best_params']

        print(f"ğŸ”§ Using best parameters:")
        print(f"   Learning rate: {best_params['lr']:.6f}")
        print(f"   Epochs: {best_params['epochs']}")

        # é‡æ–°åˆ›å»ºæ¨¡å‹
        from model import Autoencoder
        autoencoder = Autoencoder(self.config)
        model = autoencoder.build()
        self.model = model

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        if val_data is not None:
            # ä½¿ç”¨éªŒè¯æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆå¸¦æ—©åœå’ŒéªŒè¯ï¼‰
            history_dict, best_val_loss, training_time = self.train(
                train_data=train_data,
                val_data=val_data,
                model=model,
                learning_rate=best_params['lr'],
                epochs=best_params['epochs'],
                phase_name="Final Training",
                verbose=self.config.VERBOSE
            )
        else:
            # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œåªä½¿ç”¨è®­ç»ƒé›†
            print("âš ï¸ No validation data provided, training without validation")

            if isinstance(train_data, tuple):
                X_train, y_train = train_data
                history = model.fit(
                    X_train, y_train,
                    epochs=best_params['epochs'],
                    batch_size=self.config.DEFAULT_BATCH_SIZE,
                    verbose=self.config.VERBOSE,
                    callbacks=self.create_callbacks(monitor='loss', mode='min')
                )
            else:
                history = model.fit(
                    train_data,
                    epochs=best_params['epochs'],
                    verbose=self.config.VERBOSE,
                    callbacks=self.create_callbacks(monitor='loss', mode='min')
                )

            history_dict = history.history
            best_val_loss = history_dict['loss'][-1] if history_dict['loss'] else float('inf')
            training_time = time.time() - start_time

        # ä¿å­˜æœ€ç»ˆè®­ç»ƒå†å²
        self.training_history['final_train'] = {
            'history': history_dict,
            'training_time': training_time,
            'best_val_loss': best_val_loss
        }

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ï¼‰
        if self.config.SAVE_MODEL:
            model_save_path = os.path.join(self.device_output_dir, 'final_model.h5')
            model.save(model_save_path)
            print(f"âœ… Final model saved to: {model_save_path}")

        # ä¿å­˜è®­ç»ƒå†å²ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ï¼‰
        if self.config.SAVE_TRAINING_HISTORY:
            self.save_training_history()

        return best_val_loss

    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²åˆ°æ–‡ä»¶"""
        history_path = os.path.join(self.device_output_dir, 'training_history.json')

        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        history_dict = {}
        for key, value in self.training_history.items():
            if key in ['initial_train', 'final_train'] and value is not None:
                if 'history' in value:
                    # è½¬æ¢historyä¸­çš„numpyæ•°ç»„
                    converted_history = {}
                    for metric, values in value['history'].items():
                        if hasattr(values, 'tolist'):
                            converted_history[metric] = values.tolist()
                        else:
                            converted_history[metric] = values
                    value['history'] = converted_history
                history_dict[key] = value
            elif key == 'hyperparameter_tuning':
                history_dict[key] = value
            elif key == 'best_params':
                history_dict[key] = value
            elif key == 'best_val_loss':
                history_dict[key] = float(value)  # è½¬æ¢ä¸ºPython float

        with open(history_path, 'w') as f:
            json.dump(history_dict, f, indent=2, default=str)

        print(f"âœ… Training history saved to: {history_path}")

    def load_training_history(self):
        """ä»æ–‡ä»¶åŠ è½½è®­ç»ƒå†å²"""
        history_path = os.path.join(self.device_output_dir, 'training_history.json')
        if os.path.exists(history_path):
            with open(history_path, 'r') as f:
                self.training_history = json.load(f)
            print(f"âœ… Training history loaded from: {history_path}")

    def get_training_summary(self):
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'device_name': self.device_name,
            'best_params': self.training_history.get('best_params'),
            'best_val_loss': self.training_history.get('best_val_loss'),
        }

        if self.training_history.get('initial_train'):
            summary['initial_training_time'] = self.training_history['initial_train'].get('training_time')
            summary['initial_best_val_loss'] = self.training_history['initial_train'].get('best_val_loss')

        if self.training_history.get('final_train'):
            summary['final_training_time'] = self.training_history['final_train'].get('training_time')
            summary['final_best_val_loss'] = self.training_history['final_train'].get('best_val_loss')

        return summary

