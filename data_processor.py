"""
æ•°æ®å¤„ç†æ¨¡å— - åŠ è½½ã€åˆ’åˆ†ã€å½’ä¸€åŒ–
"""
import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')


class NBaIoTDataProcessor:
    """N-BaIoTæ•°æ®é›†å¤„ç†å™¨"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.scaler = StandardScaler()
        self.data_info = {}
        self._setup_random_seed()

    def _setup_random_seed(self):
        """è®¾ç½®éšæœºç§å­"""
        np.random.seed(self.config.RANDOM_SEED)

    def validate_data_root(self):
        """éªŒè¯æ•°æ®æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨"""
        if not os.path.exists(self.config.DATA_ROOT):
            raise FileNotFoundError(
                f"Data directory not found: {self.config.DATA_ROOT}\n"
                f"Please update DATA_ROOT in config.py"
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰è®¾å¤‡æ–‡ä»¶å¤¹
        device_folders = self.get_available_devices()
        if not device_folders:
            raise FileNotFoundError(
                f"No device folders found in {self.config.DATA_ROOT}"
            )

        return device_folders

    def get_available_devices(self):
        """è·å–å¯ç”¨çš„è®¾å¤‡åˆ—è¡¨ï¼ˆåŸºäºå®é™…å­˜åœ¨çš„æ–‡ä»¶å¤¹ï¼‰"""
        if not os.path.exists(self.config.DATA_ROOT):
            print(f"âš ï¸ Data directory not found: {self.config.DATA_ROOT}")
            return []

        # è·å–æ‰€æœ‰æ–‡ä»¶å¤¹
        all_folders = [f for f in os.listdir(self.config.DATA_ROOT)
                       if os.path.isdir(os.path.join(self.config.DATA_ROOT, f))]

        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦åŒ…å«benign_traffic.csvæ–‡ä»¶
        valid_devices = []
        for folder in all_folders:
            csv_path = os.path.join(self.config.DATA_ROOT, folder, "benign_traffic.csv")
            if os.path.exists(csv_path):
                valid_devices.append(folder)
            else:
                print(f"âš ï¸ Folder {folder} does not contain benign_traffic.csv")

        return valid_devices

    def load_device_data(self, device_name):
        """
        åŠ è½½å•ä¸ªè®¾å¤‡çš„æ•°æ®

        Args:
            device_name: è®¾å¤‡åç§°

        Returns:
            numpyæ•°ç»„æˆ–Noneï¼ˆå¦‚æœåŠ è½½å¤±è´¥ï¼‰
        """
        device_path = os.path.join(self.config.DATA_ROOT, device_name)
        csv_path = os.path.join(device_path, "benign_traffic.csv")

        try:
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"File not found: {csv_path}")

            # åŠ è½½CSVæ–‡ä»¶
            print(f"Loading {csv_path}...")
            df = pd.read_csv(csv_path)

            # è®°å½•æ•°æ®ä¿¡æ¯
            self.data_info[device_name] = {
                'original_shape': df.shape,
                'features': df.columns.tolist(),
                'file_path': csv_path,
                'data_type': df.dtypes.to_dict()
            }

            # æ£€æŸ¥ç‰¹å¾ç»´åº¦
            n_features = df.shape[1]
            if n_features != self.config.FEATURE_DIM:
                print(f"âš ï¸ Warning: {device_name} has {n_features} features, "
                      f"expected {self.config.FEATURE_DIM}.")

                # å¦‚æœç‰¹å¾å¤šäºé¢„æœŸï¼Œä½¿ç”¨å‰FEATURE_DIMä¸ªç‰¹å¾
                if n_features > self.config.FEATURE_DIM:
                    print(f"   Using first {self.config.FEATURE_DIM} features.")
                    df = df.iloc[:, :self.config.FEATURE_DIM]
                # å¦‚æœç‰¹å¾å°‘äºé¢„æœŸï¼Œå¡«å……é›¶å€¼
                else:
                    print(f"   Padding with zeros to {self.config.FEATURE_DIM} features.")
                    padding_cols = self.config.FEATURE_DIM - n_features
                    padding_data = np.zeros((len(df), padding_cols))
                    padding_df = pd.DataFrame(
                        padding_data,
                        columns=[f'pad_{i}' for i in range(padding_cols)]
                    )
                    df = pd.concat([df, padding_df], axis=1)

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            data = df.values.astype(np.float32)

            # æ£€æŸ¥NaNæˆ–Infå€¼
            if np.any(np.isnan(data)) or np.any(np.isinf(data)):
                print(f"âš ï¸ Warning: {device_name} data contains NaN or Inf values.")
                # ç”¨0å¡«å……NaNï¼Œç”¨æœ€å¤§/æœ€å°å€¼å¡«å……Inf
                data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=-1e6)

            print(f"âœ… Loaded {device_name}: {len(data)} samples, {data.shape[1]} features")
            return data

        except Exception as e:
            print(f"âŒ Error loading data for {device_name}: {str(e)}")
            return None

    def split_data_chronologically(self, data):
        """
        æŒ‰æ—¶é—´é¡ºåºå°†æ•°æ®ä¸‰ç­‰åˆ†

        Args:
            data: è¾“å…¥æ•°æ®

        Returns:
            DStrn, DSopt, DStst
        """
        n_samples = len(data)

        if n_samples < 3:
            raise ValueError(f"Data too small ({n_samples} samples) for three-way split")

        # è®¡ç®—åˆ’åˆ†ç‚¹
        split1 = int(n_samples * self.config.TRAIN_RATIO)
        split2 = int(n_samples * (self.config.TRAIN_RATIO + self.config.VAL_RATIO))

        # åˆ’åˆ†æ•°æ®
        DStrn = data[:split1]
        DSopt = data[split1:split2]
        DStst = data[split2:]

        print(f"ğŸ“Š Chronological split:")
        print(f"   DStrn (train): {len(DStrn)} samples ({len(DStrn) / n_samples * 100:.1f}%)")
        print(f"   DSopt (val):   {len(DSopt)} samples ({len(DSopt) / n_samples * 100:.1f}%)")
        print(f"   DStst (test):  {len(DStst)} samples ({len(DStst) / n_samples * 100:.1f}%)")

        return DStrn, DSopt, DStst

    def split_data_randomly(self, data, random_state=None):
        """
        éšæœºåˆ’åˆ†æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰

        Args:
            data: è¾“å…¥æ•°æ®
            random_state: éšæœºç§å­

        Returns:
            DStrn, DSopt, DStst
        """
        if random_state is None:
            random_state = self.config.RANDOM_SEED

        # å…ˆåˆ’åˆ†è®­ç»ƒé›†
        DStrn, temp = train_test_split(
            data,
            train_size=self.config.TRAIN_RATIO,
            random_state=random_state,
            shuffle=True
        )

        # å‰©ä½™æ•°æ®å†åˆ’åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†
        val_ratio = self.config.VAL_RATIO / (self.config.VAL_RATIO + self.config.TEST_RATIO)
        DSopt, DStst = train_test_split(
            temp,
            train_size=val_ratio,
            random_state=random_state,
            shuffle=True
        )

        print(f"ğŸ“Š Random split (seed={random_state}):")
        print(f"   DStrn (train): {len(DStrn)} samples")
        print(f"   DSopt (val):   {len(DSopt)} samples")
        print(f"   DStst (test):  {len(DStst)} samples")

        return DStrn, DSopt, DStst

    def preprocess_data(self, data, fit_scaler=False):
        """
        æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–

        Args:
            data: è¾“å…¥æ•°æ®
            fit_scaler: æ˜¯å¦æ‹Ÿåˆæ–°çš„scaler

        Returns:
            é¢„å¤„ç†åçš„æ•°æ®
        """
        if fit_scaler:
            processed_data = self.scaler.fit_transform(data)
            print(f"âœ… Fitted scaler on {len(data)} samples")
        else:
            processed_data = self.scaler.transform(data)
            print(f"âœ… Transformed {len(data)} samples using fitted scaler")

        return processed_data.astype(np.float32)

    def create_tf_datasets(self, DStrn, DSopt, batch_size=None):
        """
        åˆ›å»ºTensorFlowæ•°æ®é›†

        Args:
            DStrn: è®­ç»ƒæ•°æ®
            DSopt: éªŒè¯æ•°æ®
            batch_size: æ‰¹å¤§å°

        Returns:
            train_dataset, val_dataset
        """
        import tensorflow as tf

        if batch_size is None:
            batch_size = self.config.DEFAULT_BATCH_SIZE

        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = tf.data.Dataset.from_tensor_slices((DStrn, DStrn))
        train_dataset = train_dataset.shuffle(buffer_size=1000)
        train_dataset = train_dataset.batch(batch_size)
        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        val_dataset = tf.data.Dataset.from_tensor_slices((DSopt, DSopt))
        val_dataset = val_dataset.batch(batch_size)
        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)

        print(f"âœ… Created TensorFlow datasets:")
        print(f"   Train batches: {len(train_dataset)}")
        print(f"   Val batches: {len(val_dataset)}")
        print(f"   Batch size: {batch_size}")

        return train_dataset, val_dataset

    def create_numpy_datasets(self, DStrn, DSopt):
        """
        åˆ›å»ºNumPyæ•°æ®é›†ï¼ˆç”¨äºç®€å•è®­ç»ƒï¼‰

        Args:
            DStrn: è®­ç»ƒæ•°æ®
            DSopt: éªŒè¯æ•°æ®

        Returns:
            (X_train, y_train), (X_val, y_val)
        """
        # å¯¹äºè‡ªç¼–ç å™¨ï¼Œè¾“å…¥å’Œè¾“å‡ºç›¸åŒ
        X_train, y_train = DStrn, DStrn
        X_val, y_val = DSopt, DSopt

        print(f"âœ… Created NumPy datasets:")
        print(f"   X_train shape: {X_train.shape}")
        print(f"   X_val shape: {X_val.shape}")

        return (X_train, y_train), (X_val, y_val)

    def get_data_info(self, device_name):
        """è·å–è®¾å¤‡æ•°æ®ä¿¡æ¯"""
        return self.data_info.get(device_name, {})

    def save_scaler(self, filepath):
        """ä¿å­˜scaleråˆ°æ–‡ä»¶"""
        import joblib
        joblib.dump(self.scaler, filepath)
        print(f"âœ… Scaler saved to: {filepath}")

    def load_scaler(self, filepath):
        """ä»æ–‡ä»¶åŠ è½½scaler"""
        import joblib
        self.scaler = joblib.load(filepath)
        print(f"âœ… Scaler loaded from: {filepath}")