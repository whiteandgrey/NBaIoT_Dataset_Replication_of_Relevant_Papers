"""
æ¨¡å‹å®šä¹‰ - å¯¹ç§°ç¼–ç å™¨-è§£ç å™¨
"""
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers


class Autoencoder:
    """å¯¹ç§°è‡ªç¼–ç å™¨ç±»"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–è‡ªç¼–ç å™¨

        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.model = None
        self.encoder = None
        self.decoder = None

    def build(self, input_dim=None):
        """
        æ„å»ºè‡ªç¼–ç å™¨æ¨¡å‹

        Args:
            input_dim: è¾“å…¥ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨configä¸­çš„FEATURE_DIM

        Returns:
            æ„å»ºå¥½çš„Kerasæ¨¡å‹
        """
        if input_dim is None:
            input_dim = self.config.FEATURE_DIM

        print(f"ğŸ”¨ Building Autoencoder with input_dim={input_dim}")

        # è®¡ç®—ç¼–ç å™¨å„å±‚ç»´åº¦
        encoder_dims = [input_dim] + [int(input_dim * ratio)
                                      for ratio in self.config.ENCODER_RATIOS]

        # è®¡ç®—è§£ç å™¨å„å±‚ç»´åº¦ï¼ˆå¯¹ç§°ç»“æ„ï¼‰
        decoder_dims = [int(input_dim * ratio)
                        for ratio in self.config.DECODER_RATIOS] + [input_dim]

        # æ‰“å°æ¶æ„ä¿¡æ¯
        print(f"   Encoder architecture: {' -> '.join(map(str, encoder_dims))}")
        print(f"   Decoder architecture: {' -> '.join(map(str, decoder_dims))}")
        print(f"   Activation: {self.config.ACTIVATION}")
        print(f"   Batch normalization: {self.config.USE_BATCH_NORM}")
        print(f"   Dropout rate: {self.config.DROPOUT_RATE}")

        # è¾“å…¥å±‚
        inputs = layers.Input(shape=(input_dim,))
        x = inputs

        # ============ ç¼–ç å™¨éƒ¨åˆ† ============
        encoder_layers = []

        for i in range(len(encoder_dims) - 1):
            # å…¨è¿æ¥å±‚
            x = layers.Dense(
                encoder_dims[i + 1],
                activation=None,  # å…ˆä¸åŠ æ¿€æ´»å‡½æ•°ï¼Œç”¨äºBatchNorm
                kernel_regularizer=regularizers.l2(self.config.L2_REGULARIZATION),
                name=f'encoder_dense_{i}'
            )(x)

            # Batch Normalization
            if self.config.USE_BATCH_NORM:
                x = layers.BatchNormalization(name=f'encoder_bn_{i}')(x)

            # æ¿€æ´»å‡½æ•°
            if self.config.ACTIVATION == 'relu':
                x = layers.ReLU(name=f'encoder_relu_{i}')(x)
            elif self.config.ACTIVATION == 'leaky_relu':
                x = layers.LeakyReLU(alpha=0.1, name=f'encoder_leaky_relu_{i}')(x)
            elif self.config.ACTIVATION == 'tanh':
                x = layers.Activation('tanh', name=f'encoder_tanh_{i}')(x)
            elif self.config.ACTIVATION == 'sigmoid':
                x = layers.Activation('sigmoid', name=f'encoder_sigmoid_{i}')(x)
            else:
                x = layers.ReLU(name=f'encoder_relu_{i}')(x)

            # Dropout
            if self.config.DROPOUT_RATE > 0:
                x = layers.Dropout(self.config.DROPOUT_RATE,
                                   name=f'encoder_dropout_{i}')(x)

            encoder_layers.append(x)

        # ç¼–ç å™¨è¾“å‡ºï¼ˆæ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼‰
        latent_representation = encoder_layers[-1]

        # ============ è§£ç å™¨éƒ¨åˆ† ============
        x = latent_representation

        for i in range(len(decoder_dims) - 1):
            # å…¨è¿æ¥å±‚
            x = layers.Dense(
                decoder_dims[i + 1],
                activation=None,  # å…ˆä¸åŠ æ¿€æ´»å‡½æ•°ï¼Œç”¨äºBatchNorm
                kernel_regularizer=regularizers.l2(self.config.L2_REGULARIZATION),
                name=f'decoder_dense_{i}'
            )(x)

            # æœ€åä¸€å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰çš„ç‰¹æ®Šå¤„ç†
            if i == len(decoder_dims) - 2:  # è¾“å‡ºå±‚
                if self.config.OUTPUT_ACTIVATION:
                    x = layers.Activation(self.config.OUTPUT_ACTIVATION,
                                          name='output_activation')(x)
            else:
                # Batch Normalizationï¼ˆè¾“å‡ºå±‚å‰ä¸€å±‚ä¸ä½¿ç”¨ï¼‰
                if self.config.USE_BATCH_NORM:
                    x = layers.BatchNormalization(name=f'decoder_bn_{i}')(x)

                # æ¿€æ´»å‡½æ•°
                if self.config.ACTIVATION == 'relu':
                    x = layers.ReLU(name=f'decoder_relu_{i}')(x)
                elif self.config.ACTIVATION == 'leaky_relu':
                    x = layers.LeakyReLU(alpha=0.1, name=f'decoder_leaky_relu_{i}')(x)
                elif self.config.ACTIVATION == 'tanh':
                    x = layers.Activation('tanh', name=f'decoder_tanh_{i}')(x)
                elif self.config.ACTIVATION == 'sigmoid':
                    x = layers.Activation('sigmoid', name=f'decoder_sigmoid_{i}')(x)
                else:
                    x = layers.ReLU(name=f'decoder_relu_{i}')(x)

                # Dropoutï¼ˆè¾“å‡ºå±‚å‰ä¸€å±‚ä¸ä½¿ç”¨ï¼‰
                if self.config.DROPOUT_RATE > 0:
                    x = layers.Dropout(self.config.DROPOUT_RATE,
                                       name=f'decoder_dropout_{i}')(x)

        # è§£ç å™¨è¾“å‡º
        outputs = x

        # ============ åˆ›å»ºæ¨¡å‹ ============
        self.model = models.Model(inputs=inputs, outputs=outputs, name='autoencoder')

        # åˆ›å»ºç¼–ç å™¨æ¨¡å‹
        self.encoder = models.Model(inputs=inputs, outputs=latent_representation,
                                    name='encoder')

        # åˆ›å»ºè§£ç å™¨æ¨¡å‹
        latent_input = layers.Input(shape=(encoder_dims[-1],))
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè§£ç å™¨ Dense å±‚ï¼ˆä»¥ 'decoder_dense_' å¼€å¤´çš„å±‚ï¼‰
        decoder_output = None
        first_decoder_layer_idx = None
        
        for idx, layer in enumerate(self.model.layers):
            if layer.name.startswith('decoder_dense_'):
                first_decoder_layer_idx = idx
                break
        
        if first_decoder_layer_idx is not None:
            decoder_output = self.model.layers[first_decoder_layer_idx](latent_input)
            # å¤„ç†å‰©ä½™çš„è§£ç å™¨å±‚
            for layer in self.model.layers[first_decoder_layer_idx + 1:]:
                decoder_output = layer(decoder_output)
        else:
            raise ValueError("Could not find decoder layers in the model")

        self.decoder = models.Model(inputs=latent_input, outputs=decoder_output,
                                    name='decoder')

        # ç¼–è¯‘æ¨¡å‹
        self.compile()

        return self.model

    def compile(self, learning_rate=None):
        """
        ç¼–è¯‘æ¨¡å‹

        Args:
            learning_rate: å­¦ä¹ ç‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨configä¸­çš„é»˜è®¤å€¼
        """
        if learning_rate is None:
            learning_rate = self.config.DEFAULT_LEARNING_RATE

        if self.config.OPTIMIZER.lower() == 'adam':
            optimizer = tf.keras.optimizers.Adam(
                learning_rate=learning_rate,
                beta_1=self.config.BETA_1,
                beta_2=self.config.BETA_2,
                epsilon=self.config.EPSILON
            )
        elif self.config.OPTIMIZER.lower() == 'rmsprop':
            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
        elif self.config.OPTIMIZER.lower() == 'sgd':
            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
        else:
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

        self.model.compile(
            optimizer=optimizer,
            loss='mse',  # å‡æ–¹è¯¯å·®
            metrics=['mae']  # å¹³å‡ç»å¯¹è¯¯å·®
        )

        print(f"âœ… Model compiled with {self.config.OPTIMIZER} optimizer, "
              f"LR={learning_rate:.6f}")

    def summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        if self.model:
            self.model.summary()
        else:
            print("âš ï¸ Model not built yet. Call build() first.")

    def get_model(self):
        """è·å–æ¨¡å‹"""
        return self.model

    def get_encoder(self):
        """è·å–ç¼–ç å™¨"""
        return self.encoder

    def get_decoder(self):
        """è·å–è§£ç å™¨"""
        return self.decoder

    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        if self.model:
            self.model.save(filepath)
            print(f"âœ… Model saved to: {filepath}")
        else:
            print("âš ï¸ No model to save")

    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        self.model = tf.keras.models.load_model(filepath)

        # é‡å»ºç¼–ç å™¨å’Œè§£ç å™¨
        inputs = self.model.input
        # æ‰¾åˆ°æ½œåœ¨å±‚ï¼ˆç¼–ç å™¨çš„æœ€åä¸€å±‚ï¼‰
        encoder_output = None
        for layer in self.model.layers:
            if 'encoder' in layer.name and layer.name.endswith('_relu_3'):
                encoder_output = layer.output
                break

        if encoder_output is None:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šåç§°ï¼Œä½¿ç”¨ä¸­é—´å±‚
            num_layers = len(self.model.layers)
            latent_layer_idx = num_layers // 2 - 1
            encoder_output = self.model.layers[latent_layer_idx].output

        self.encoder = models.Model(inputs=inputs, outputs=encoder_output,
                                    name='encoder')

        print(f"âœ… Model loaded from: {filepath}")
        return self.model